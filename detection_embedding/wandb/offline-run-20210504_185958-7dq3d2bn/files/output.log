GPU available: False, used: False
TPU available: False, using: 0 TPU cores

  | Name             | Type             | Params
------------------------------------------------------
0 | anchor_generator | AnchorGenerator  | 0     
1 | backbone         | BackboneWithFPN  | 14.3 M
2 | model            | RetinaNet        | 23.2 M
3 | bbone            | ResNet           | 11.7 M
4 | extractor        | Sequential       | 11.2 M
5 | teacher_model    | RecognitionModel | 11.3 M
6 | tm               | Sequential       | 11.2 M
------------------------------------------------------
46.2 M    Trainable params
0         Non-trainable params
46.2 M    Total params
184.650   Total estimated model params size (MB)
/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/Users/emilecarron/dataset/SKU110K/images/val_169.jpg
Validation sanity check:  50%|█████     | 1/2 [00:11<00:11, 11.85s/it]/Users/emilecarron/dataset/SKU110K/images/val_33.jpg
Validation sanity check: 100%|██████████| 2/2 [00:23<00:00, 11.81s/it]/Users/emilecarron/dataset/SKU110K/images/val_280.jpg
/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
                                                                      Training: 0it [00:00, ?it/s]Training:   0%|          | 0/534 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/534 [00:00<?, ?it/s] /Users/emilecarron/dataset/SKU110K/images/train_0.jpg
/Users/emilecarron/dataset/SKU110K/images/train_1.jpg
> /Users/emilecarron/projects/thesis/detectie/retinanet.py(204)training_step()
-> height = idx[3]-idx[1]
(Pdb) [{'boxes': tensor([[ 762.5218,    0.0000,  920.8135,   88.9946],
        [ 789.3171,  482.9950,  905.6495,  564.3958],
        [ 870.7963,  495.2517,  944.2409,  537.2456],
        [ 926.6730,  482.7513, 1004.0055,  535.0462],
        [ 944.2171,  460.0695, 1007.7176,  502.8987],
        [ 986.6917,  443.9615, 1055.3816,  489.5987],
        [1036.0276,  439.9605, 1103.8554,  485.3254],
        [1097.9202,  434.7993, 1140.6822,  460.4629],
        [1102.6536,  398.8703, 1168.5932,  441.8149],
        [1144.3278,  392.5987, 1215.3879,  440.7616],
        [1172.4763,  379.1789, 1244.4437,  428.3766],
        [1197.9197,  368.9990, 1261.9237,  415.8691],
        [1213.1197,  354.4903, 1291.9271,  406.6898],
        [1251.4879,  345.2369, 1318.8620,  390.0843],
        [1263.8379,  306.8135, 1333.0000,  376.2769],
        [1230.4642,  230.5283, 1312.7130,  288.1641],
        [ 795.3923,  417.2290,  881.0444,  475.1912],
        [ 810.7253,  384.9660,  894.3587,  440.9812],
        [ 293.1095,   21.1278,  421.6994,  105.0429],
        [ 375.2490,   28.5054,  448.2258,   78.6766],
        [ 414.5007,   24.4602,  483.6679,   67.4419],
        [ 440.9891,    0.0000,  528.5365,   48.6856],
        [ 493.5336,    0.0000,  602.1426,   20.9823],
        [ 534.6188,    0.0000,  650.6069,    3.3492],
        [ 701.1736,  466.0248,  867.1674,  585.1362],
        [1048.2136,  279.1544, 1156.3191,  349.3049],
        [ 927.3854,  330.0997, 1013.8213,  385.2225],
        [ 913.1069,  367.6008, 1003.9769,  427.9591],
        [ 866.8442,  379.6209,  952.2755,  436.5312],
        [ 652.0799,  485.5194,  815.0741,  601.2984],
        [ 638.6065,  527.7856,  778.5584,  628.5712],
        [ 615.5857,  578.5654,  738.8175,  655.0359],
        [1006.9079,  564.6823, 1175.4334,  678.3261],
        [ 907.2574,  556.0190, 1034.4508,  643.1411],
        [ 897.4844,  618.8887, 1058.1890,  728.1452],
        [ 828.5309,  629.1684,  998.6311,  749.7637],
        [ 752.5450,  633.8151,  900.4152,  736.1658],
        [ 694.0131,  660.5833,  869.4452,  781.9269],
        [1059.1056,  534.0128, 1218.3568,  647.3003],
        [ 631.3700,  685.2748,  809.5809,  800.0000],
        [ 574.6503,  799.2351,  702.1856,  800.0000],
        [ 517.4055,  721.0400,  617.7257,  782.0439],
        [ 351.3287,  796.0780,  458.2538,  800.0000],
        [ 662.5081,   27.9736,  834.9785,  134.1172],
        [ 609.1519,  727.7819,  721.7699,  800.0000],
        [ 233.2737,   46.0202,  367.8296,  134.2513],
        [1107.3657,  517.0026, 1259.3399,  627.9453],
        [1187.5814,  461.8797, 1333.0000,  566.7290],
        [ 487.1312,  556.4738,  600.4545,  629.0203],
        [ 552.4438,  605.4790,  666.1923,  678.9548],
        [ 503.4119,  632.9143,  612.8616,  703.4425],
        [ 437.4462,  647.6127,  561.1815,  728.1240],
        [ 382.3818,  663.8660,  501.3172,  747.2580],
        [ 276.3212,  648.0531,  464.3551,  776.8371],
        [1146.4487,  499.1485, 1304.1799,  611.7689],
        [ 217.5185,  671.0798,  382.2940,  786.5795],
        [ 226.9212,  737.4075,  314.7180,  794.0826],
        [1330.5756,  457.5308, 1333.0000,  527.2366],
        [1271.4333,  459.0474, 1333.0000,  567.1951],
        [ 181.1905,  702.2254,  266.2655,  755.7960],
        [ 170.7494,   71.6582,  310.7274,  162.1627],
        [  59.2787,  127.1645,  185.7358,  210.7803],
        [ 759.1400,  146.6033,  845.8229,  209.4745],
        [ 798.2396,  120.7516,  926.2737,  204.2106],
        [ 859.0682,   90.8219,  991.5602,  176.3434],
        [ 924.5019,   65.4904, 1056.9535,  153.0103],
        [ 981.8534,   45.0564, 1106.4272,  129.0129],
        [   0.0000,  346.7653,   27.8172,  421.3201],
        [  10.5485,  323.1599,   85.4989,  374.5156],
        [  33.3153,  300.9656,  135.7540,  370.9635],
        [ 709.3647,  162.7056,  836.8145,  246.1204],
        [1194.3689,  247.2976, 1290.7917,  309.4561],
        [ 842.2483,    0.0000,  994.1535,   53.6400],
        [  77.5616,  264.2142,  183.5105,  337.2381],
        [ 114.3195,  249.4332,  220.7672,  323.7371],
        [ 155.0531,  231.7730,  262.2893,  305.9985],
        [ 194.9817,  215.0179,  301.7239,  290.7244],
        [ 249.2173,  208.5125,  353.8291,  276.8114],
        [ 280.0852,  175.3202,  380.3507,  247.0171],
        [ 319.9900,  162.9053,  423.2623,  233.5874],
        [ 360.0610,  137.9202,  468.5559,  213.7592],
        [ 391.9193,  116.1379,  508.0024,  196.8986],
        [1101.0337,  258.4753, 1188.9896,  314.2653],
        [ 697.7438,  224.8256,  775.6796,  271.2310],
        [ 448.6059,  199.1507,  635.5473,  321.0887],
        [ 394.3989,  257.0759,  549.8429,  356.0656],
        [   0.0000,  629.7160,  107.7649,  712.6662],
        [  42.8868,  598.7686,  179.0910,  694.3018],
        [  96.8350,  575.3400,  236.9365,  669.7186],
        [ 186.0187,  521.7579,  323.1847,  624.6103],
        [ 205.3655,  498.5630,  365.1605,  613.8930],
        [ 252.8441,  455.9376,  422.4978,  571.6682],
        [ 319.6221,  428.9827,  487.7724,  545.2206],
        [ 397.1683,  422.6712,  547.9526,  529.5901],
        [ 459.0276,  400.9857,  621.8366,  510.9535],
        [ 532.2870,  386.3045,  675.0366,  483.4370],
        [ 588.9640,  362.9671,  725.7239,  458.9564],
        [ 602.4778,  318.3691,  741.8407,  416.3496],
        [ 687.8211,  310.1119,  820.3297,  396.8079],
        [ 745.1276,  278.4091,  890.7319,  371.0645],
        [ 815.5013,  268.4202,  915.3298,  340.7742],
        [ 837.5247,  252.0699,  946.5920,  329.5395],
        [ 874.7647,  218.5497, 1022.9721,  313.1963],
        [ 958.3648,  182.5280, 1105.8858,  277.1917],
        [1019.6256,  156.1845, 1124.4915,  227.8843],
        [1122.1165,  134.4107, 1227.8873,  214.2536],
        [   0.0000,  473.0663,   47.5798,  563.2031],
        [   0.0000,  411.7032,   57.5713,  466.8346],
        [   1.6257,  443.7224,  122.9386,  525.2928],
        [  64.2332,  430.7944,  184.9404,  508.3852],
        [ 121.9766,  394.0985,  261.1732,  480.3341],
        [ 190.1938,  338.3199,  320.9473,  431.9916],
        [ 261.9094,  321.6418,  389.2856,  404.7060],
        [ 319.2895,  289.9560,  472.0450,  388.1903],
        [ 538.9961,  102.1037,  671.0796,  187.8704],
        [ 113.5736,   99.5525,  245.8495,  186.7830],
        [   0.0000,  227.0366,    5.1501,  290.2299],
        [  32.8972,  169.0116,  128.7642,  230.7140],
        [   0.0000,  187.6112,   84.5820,  252.4550],
        [   0.0000,  211.8608,   39.7301,  271.0517],
        [ 601.5370,   74.4717,  741.4984,  163.8018]], dtype=torch.float64), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1]), 'embedding': tensor([[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],
        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],
        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],
        ...,
        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],
        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],
        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000]],
       dtype=torch.float64, requires_grad=True)}]
(Pdb) *** NameError: name 'embedding' is not defined
(Pdb) tensor([[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],
        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],
        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],
        ...,
        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],
        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],
        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000]],
       dtype=torch.float64, requires_grad=True)
(Pdb) torch.Size([141, 512])
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(203)training_step()
-> import pdb; pdb.set_trace()
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(204)training_step()
-> height = idx[3]-idx[1]
(Pdb) *** RuntimeError: A view was created in no_grad mode and its base or another view of its base has been modified inplace with grad mode enabled. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.
(Pdb) tensor([0.0000e+00, 0.0000e+00, 8.1422e-02, 0.0000e+00, 8.1437e-03, 0.0000e+00,
        0.0000e+00, 3.1500e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1132e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.2023e-01, 1.3834e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 5.0248e-02, 0.0000e+00, 1.8641e-01, 0.0000e+00,
        0.0000e+00, 1.5495e-03, 0.0000e+00, 5.2666e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.6659e-02, 0.0000e+00, 5.6384e-02, 1.8653e+00, 1.0817e-01,
        1.5001e-01, 1.1286e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1973e-01, 4.2255e-03, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0853e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5698e-02, 1.3184e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.5047e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.2590e-02, 4.0447e-02, 6.7089e-02, 1.0744e-01, 0.0000e+00,
        0.0000e+00, 2.2177e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 9.0528e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.9829e-01, 1.0752e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 5.8210e-01, 0.0000e+00, 0.0000e+00, 6.6094e-01, 0.0000e+00,
        0.0000e+00, 4.6282e-02, 3.1810e-01, 0.0000e+00, 3.8749e-01, 0.0000e+00,
        0.0000e+00, 1.0340e-01, 6.3271e-02, 0.0000e+00, 0.0000e+00, 1.4651e-01,
        1.6821e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.6843e-03, 0.0000e+00, 0.0000e+00, 1.0024e-01, 0.0000e+00,
        0.0000e+00, 2.5926e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6043e-01,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9535e-01, 4.2407e-01, 0.0000e+00,
        0.0000e+00, 9.2363e-02, 0.0000e+00, 0.0000e+00, 6.0287e-01, 1.1561e-01,
        3.1146e-03, 0.0000e+00, 0.0000e+00, 1.0814e-02, 0.0000e+00, 0.0000e+00,
        4.8645e-03, 3.4648e-02, 0.0000e+00, 5.5383e-03, 1.7685e-03, 6.0264e-02,
        0.0000e+00, 0.0000e+00, 5.6097e-01, 6.2585e-01, 0.0000e+00, 0.0000e+00,
        2.1514e-02, 1.1103e-02, 0.0000e+00, 0.0000e+00, 4.8168e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2844e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 6.1270e-01, 0.0000e+00, 3.2983e-01, 2.3759e-01,
        0.0000e+00, 1.3635e-01, 0.0000e+00, 0.0000e+00, 1.7720e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        2.4559e-01, 0.0000e+00, 2.1708e-01, 2.0400e-03, 2.2505e-02, 0.0000e+00,
        0.0000e+00, 1.8510e-01, 4.0032e-03, 0.0000e+00, 1.0183e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5286e-01, 3.5982e-01,
        1.6774e-01, 1.0153e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2633e-03, 6.1272e-01,
        7.9349e-02, 1.6656e-01, 1.7400e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.4967e-02, 0.0000e+00, 1.0928e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8288e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9440e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 7.1416e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.4614e+00, 0.0000e+00, 2.5988e-01, 0.0000e+00,
        0.0000e+00, 3.8399e-01, 0.0000e+00, 6.6126e-03, 4.8903e-01, 2.9128e-01,
        1.6876e+00, 0.0000e+00, 1.2321e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.1980e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0544e-01,
        5.8254e-02, 0.0000e+00, 2.6250e-02, 0.0000e+00, 2.6064e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.9804e-01, 1.9727e-01, 1.1922e+00, 9.2398e-01,
        0.0000e+00, 0.0000e+00, 1.1650e-01, 1.1440e-01, 0.0000e+00, 1.3874e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6160e-03, 0.0000e+00, 1.3464e-01,
        8.3135e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0074e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5898e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.5019e+00, 1.1074e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.7212e-01, 0.0000e+00, 7.5474e-02, 4.4250e-02, 0.0000e+00,
        0.0000e+00, 4.2063e-01, 0.0000e+00, 4.1706e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6640e-01, 1.0300e-02, 0.0000e+00,
        1.2907e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3299e-02, 0.0000e+00,
        3.0012e-02, 3.3579e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.9952e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1617e-01,
        0.0000e+00, 8.0325e-02, 3.1068e-01, 0.0000e+00, 0.0000e+00, 8.0736e-01,
        0.0000e+00, 0.0000e+00, 7.1797e-02, 1.5239e+00, 0.0000e+00, 0.0000e+00,
        2.8816e-01, 3.2989e-01, 1.2183e-01, 0.0000e+00, 3.3866e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 5.1128e-02, 0.0000e+00, 8.0966e-03, 0.0000e+00,
        1.5372e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0719e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        6.1372e-02, 2.1648e-02, 2.2451e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.5174e-01, 0.0000e+00, 0.0000e+00, 2.9044e-01, 0.0000e+00, 1.5145e-02,
        1.5868e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        4.9120e-01, 0.0000e+00, 5.7042e-01, 0.0000e+00, 2.9917e-04, 8.7375e-02,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2582e-01,
        6.0559e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.4061e-01, 0.0000e+00, 0.0000e+00, 7.4329e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 6.5250e-02, 0.0000e+00, 3.3522e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.1932e-01, 0.0000e+00, 0.0000e+00, 7.3601e-01, 1.1039e+00,
        0.0000e+00, 0.0000e+00, 2.7959e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00])
(Pdb) tensor(0.)
(Pdb) tensor(0.)
(Pdb) tensor(0.0081)
(Pdb) (Pdb) tensor(0.0081, dtype=torch.float64, requires_grad=True)
(Pdb) tensor(0.0814, dtype=torch.float64, requires_grad=True)
(Pdb) (Pdb) tensor([0.0000e+00, 8.1437e-03, 8.1422e-02, 0.0000e+00, 8.1437e-03, 0.0000e+00,
        0.0000e+00, 3.1500e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1132e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.2023e-01, 1.3834e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 5.0248e-02, 0.0000e+00, 1.8641e-01, 0.0000e+00,
        0.0000e+00, 1.5495e-03, 0.0000e+00, 5.2666e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.6659e-02, 0.0000e+00, 5.6384e-02, 1.8653e+00, 1.0817e-01,
        1.5001e-01, 1.1286e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1973e-01, 4.2255e-03, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0853e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5698e-02, 1.3184e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.5047e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.2590e-02, 4.0447e-02, 6.7089e-02, 1.0744e-01, 0.0000e+00,
        0.0000e+00, 2.2177e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 9.0528e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.9829e-01, 1.0752e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 5.8210e-01, 0.0000e+00, 0.0000e+00, 6.6094e-01, 0.0000e+00,
        0.0000e+00, 4.6282e-02, 3.1810e-01, 0.0000e+00, 3.8749e-01, 0.0000e+00,
        0.0000e+00, 1.0340e-01, 6.3271e-02, 0.0000e+00, 0.0000e+00, 1.4651e-01,
        1.6821e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.6843e-03, 0.0000e+00, 0.0000e+00, 1.0024e-01, 0.0000e+00,
        0.0000e+00, 2.5926e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6043e-01,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9535e-01, 4.2407e-01, 0.0000e+00,
        0.0000e+00, 9.2363e-02, 0.0000e+00, 0.0000e+00, 6.0287e-01, 1.1561e-01,
        3.1146e-03, 0.0000e+00, 0.0000e+00, 1.0814e-02, 0.0000e+00, 0.0000e+00,
        4.8645e-03, 3.4648e-02, 0.0000e+00, 5.5383e-03, 1.7685e-03, 6.0264e-02,
        0.0000e+00, 0.0000e+00, 5.6097e-01, 6.2585e-01, 0.0000e+00, 0.0000e+00,
        2.1514e-02, 1.1103e-02, 0.0000e+00, 0.0000e+00, 4.8168e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2844e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 6.1270e-01, 0.0000e+00, 3.2983e-01, 2.3759e-01,
        0.0000e+00, 1.3635e-01, 0.0000e+00, 0.0000e+00, 1.7720e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        2.4559e-01, 0.0000e+00, 2.1708e-01, 2.0400e-03, 2.2505e-02, 0.0000e+00,
        0.0000e+00, 1.8510e-01, 4.0032e-03, 0.0000e+00, 1.0183e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5286e-01, 3.5982e-01,
        1.6774e-01, 1.0153e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2633e-03, 6.1272e-01,
        7.9349e-02, 1.6656e-01, 1.7400e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.4967e-02, 0.0000e+00, 1.0928e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8288e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9440e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 7.1416e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.4614e+00, 0.0000e+00, 2.5988e-01, 0.0000e+00,
        0.0000e+00, 3.8399e-01, 0.0000e+00, 6.6126e-03, 4.8903e-01, 2.9128e-01,
        1.6876e+00, 0.0000e+00, 1.2321e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.1980e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0544e-01,
        5.8254e-02, 0.0000e+00, 2.6250e-02, 0.0000e+00, 2.6064e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.9804e-01, 1.9727e-01, 1.1922e+00, 9.2398e-01,
        0.0000e+00, 0.0000e+00, 1.1650e-01, 1.1440e-01, 0.0000e+00, 1.3874e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6160e-03, 0.0000e+00, 1.3464e-01,
        8.3135e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0074e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5898e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.5019e+00, 1.1074e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.7212e-01, 0.0000e+00, 7.5474e-02, 4.4250e-02, 0.0000e+00,
        0.0000e+00, 4.2063e-01, 0.0000e+00, 4.1706e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6640e-01, 1.0300e-02, 0.0000e+00,
        1.2907e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3299e-02, 0.0000e+00,
        3.0012e-02, 3.3579e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.9952e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1617e-01,
        0.0000e+00, 8.0325e-02, 3.1068e-01, 0.0000e+00, 0.0000e+00, 8.0736e-01,
        0.0000e+00, 0.0000e+00, 7.1797e-02, 1.5239e+00, 0.0000e+00, 0.0000e+00,
        2.8816e-01, 3.2989e-01, 1.2183e-01, 0.0000e+00, 3.3866e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 5.1128e-02, 0.0000e+00, 8.0966e-03, 0.0000e+00,
        1.5372e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0719e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        6.1372e-02, 2.1648e-02, 2.2451e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.5174e-01, 0.0000e+00, 0.0000e+00, 2.9044e-01, 0.0000e+00, 1.5145e-02,
        1.5868e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        4.9120e-01, 0.0000e+00, 5.7042e-01, 0.0000e+00, 2.9917e-04, 8.7375e-02,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2582e-01,
        6.0559e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.4061e-01, 0.0000e+00, 0.0000e+00, 7.4329e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 6.5250e-02, 0.0000e+00, 3.3522e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.1932e-01, 0.0000e+00, 0.0000e+00, 7.3601e-01, 1.1039e+00,
        0.0000e+00, 0.0000e+00, 2.7959e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00], dtype=torch.float64, requires_grad=True)
(Pdb) Epoch 0:   0%|          | 0/534 [04:30<?, ?it/s]
Traceback (most recent call last):
  File "train.py", line 78, in <module>
    main(args)
  File "train.py", line 59, in main
    trainer.fit(model, dm)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 499, in fit
    self.dispatch()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 546, in dispatch
    self.accelerator.start_training(self)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 114, in start_training
    self._results = trainer.run_train()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 637, in run_train
    self.train_loop.run_training_epoch()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 492, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 654, in run_training_batch
    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 433, in optimizer_step
    using_lbfgs=is_lbfgs,
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1390, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py", line 214, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py", line 134, in __optimizer_step
    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 277, in optimizer_step
    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 282, in run_optimizer_step
    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=lambda_closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/optim/optimizer.py", line 89, in wrapper
    return func(*args, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/optim/adam.py", line 66, in step
    loss = closure()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 649, in train_step_and_backward_closure
    split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 742, in training_step_and_backward
    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 293, in training_step
    training_step_output = self.trainer.accelerator.training_step(args)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 156, in training_step
    return self.training_type_plugin.training_step(*args)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 125, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/Users/emilecarron/projects/thesis/detectie/retinanet.py", line 204, in training_step
    height = idx[3]-idx[1]
  File "/Users/emilecarron/projects/thesis/detectie/retinanet.py", line 204, in training_step
    height = idx[3]-idx[1]
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/bdb.py", line 51, in trace_dispatch
    return self.dispatch_line(frame)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/bdb.py", line 70, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
