GPU available: False, used: False
TPU available: False, using: 0 TPU cores

  | Name             | Type             | Params
------------------------------------------------------
0 | anchor_generator | AnchorGenerator  | 0     
1 | backbone         | BackboneWithFPN  | 14.3 M
2 | model            | RetinaNet        | 23.2 M
3 | bbone            | ResNet           | 11.7 M
4 | extractor        | Sequential       | 11.2 M
5 | teacher_model    | RecognitionModel | 11.3 M
6 | tm               | Sequential       | 11.2 M
------------------------------------------------------
46.2 M    Trainable params
0         Non-trainable params
46.2 M    Total params
184.650   Total estimated model params size (MB)
/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/Users/emilecarron/dataset/SKU110K/images/val_169.jpg
Validation sanity check:  50%|█████     | 1/2 [00:12<00:12, 12.02s/it]/Users/emilecarron/dataset/SKU110K/images/val_33.jpg
Validation sanity check: 100%|██████████| 2/2 [00:23<00:00, 11.99s/it]/Users/emilecarron/dataset/SKU110K/images/val_280.jpg
/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
                                                                      Training: 0it [00:00, ?it/s]Training:   0%|          | 0/534 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/534 [00:00<?, ?it/s] /Users/emilecarron/dataset/SKU110K/images/train_0.jpg
/Users/emilecarron/dataset/SKU110K/images/train_1.jpg
> /Users/emilecarron/projects/thesis/detectie/retinanet.py(233)training_step()
-> print(y[0]['embedding'][0])
(Pdb) tensor([0.0000e+00, 0.0000e+00, 5.6486e-01, 0.0000e+00, 9.7720e-04, 0.0000e+00,
        0.0000e+00, 3.3600e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5610e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.0952e-02, 1.3630e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 4.4340e-01, 0.0000e+00, 2.7884e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0037e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.2587e-01, 0.0000e+00, 0.0000e+00, 1.7111e+00, 0.0000e+00,
        4.1283e-01, 1.6071e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5771e-01, 1.4905e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3910e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.3436e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.8732e-03, 0.0000e+00, 0.0000e+00, 3.5155e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 9.5058e-03, 0.0000e+00, 1.5320e-02, 3.2439e-01, 0.0000e+00,
        0.0000e+00, 7.9584e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.6707e-02, 0.0000e+00, 8.3296e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 7.2825e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 4.1724e-01, 2.4584e-01, 3.8423e-01, 7.5054e-03,
        0.0000e+00, 1.1839e+00, 4.7646e-02, 0.0000e+00, 1.0028e+00, 0.0000e+00,
        0.0000e+00, 1.6598e-02, 3.6233e-01, 0.0000e+00, 2.5988e-01, 7.7398e-03,
        1.0546e+00, 9.3730e-01, 5.6525e-01, 0.0000e+00, 0.0000e+00, 2.6005e-01,
        6.7497e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1846e-01, 0.0000e+00,
        0.0000e+00, 2.0009e-01, 0.0000e+00, 0.0000e+00, 1.0877e+00, 0.0000e+00,
        0.0000e+00, 3.7794e-01, 0.0000e+00, 3.9357e-03, 3.3493e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.2152e-02, 0.0000e+00, 3.6205e-01, 0.0000e+00,
        0.0000e+00, 2.9279e-01, 0.0000e+00, 0.0000e+00, 1.9994e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        6.2435e-02, 1.9683e-02, 0.0000e+00, 3.8492e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.2627e+00, 6.8352e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.7157e-01, 0.0000e+00, 0.0000e+00, 1.2075e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1736e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.1952e-01, 0.0000e+00, 7.0322e-01, 4.7335e-01,
        0.0000e+00, 6.2843e-01, 0.0000e+00, 9.5868e-02, 1.4061e+00, 3.2528e-01,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        8.7401e-02, 0.0000e+00, 1.3968e-01, 0.0000e+00, 3.7667e-02, 0.0000e+00,
        5.1813e-01, 4.7483e-01, 0.0000e+00, 0.0000e+00, 2.9226e-01, 0.0000e+00,
        0.0000e+00, 1.2305e-01, 0.0000e+00, 2.0261e-01, 6.0026e-02, 1.0085e+00,
        0.0000e+00, 1.4452e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5717e-01, 1.5029e+00,
        4.3539e-01, 7.8917e-02, 1.9142e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.6282e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 4.8178e-03, 0.0000e+00, 0.0000e+00, 4.4910e-01, 0.0000e+00,
        0.0000e+00, 4.1372e-01, 0.0000e+00, 0.0000e+00, 1.6993e+00, 0.0000e+00,
        3.5446e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1119e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.9899e+00, 0.0000e+00, 1.6027e+00, 7.8987e-04,
        4.1296e-02, 1.4273e+00, 0.0000e+00, 9.0462e-02, 2.1265e-01, 4.7015e-01,
        1.4493e+00, 0.0000e+00, 1.0066e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.7492e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.8558e-01, 0.0000e+00, 6.5020e-01, 0.0000e+00, 5.6634e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.3163e+00, 5.7428e-01, 5.8331e-01, 5.2840e-01,
        0.0000e+00, 3.5018e-02, 1.0373e+00, 7.5098e-02, 4.2691e-02, 1.4728e+00,
        9.1642e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4155e-01,
        9.9553e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4459e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.7028e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 8.9671e-01, 0.0000e+00, 1.3114e-02, 1.9441e-01, 0.0000e+00,
        1.4403e-02, 4.8905e-01, 0.0000e+00, 7.1332e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3628e-01, 0.0000e+00, 0.0000e+00,
        1.7035e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6735e-02, 0.0000e+00,
        2.4069e-01, 8.2618e-01, 0.0000e+00, 8.1662e-02, 0.0000e+00, 1.0489e-02,
        0.0000e+00, 4.1119e+00, 0.0000e+00, 2.7507e-01, 0.0000e+00, 3.9473e-01,
        0.0000e+00, 3.3967e-02, 1.0322e+00, 0.0000e+00, 0.0000e+00, 1.2159e-03,
        2.0854e-01, 0.0000e+00, 0.0000e+00, 1.4869e+00, 0.0000e+00, 0.0000e+00,
        1.7695e-01, 3.3024e-02, 6.1789e-01, 0.0000e+00, 2.3612e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.4368e-03, 0.0000e+00, 1.5623e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2048e-02, 3.7135e-02,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6261e-02, 0.0000e+00, 0.0000e+00,
        1.1605e-01, 3.7282e-01, 2.6238e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.4303e+00, 0.0000e+00, 0.0000e+00, 4.9590e-01, 5.2212e-02, 0.0000e+00,
        7.7748e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1797e-02, 2.2745e-02,
        1.3561e+00, 1.3715e-01, 7.4849e-01, 0.0000e+00, 9.1447e-02, 3.2168e-01,
        0.0000e+00, 1.8156e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5158e-01,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.7393e-01, 0.0000e+00, 0.0000e+00, 4.0947e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.5988e-02, 0.0000e+00, 4.6531e-02, 6.3466e-01, 1.0928e+00,
        0.0000e+00, 0.0000e+00, 3.3352e-01, 0.0000e+00, 0.0000e+00, 3.9178e-02,
        0.0000e+00, 0.0000e+00], dtype=torch.float64, requires_grad=True)
> /Users/emilecarron/projects/thesis/detectie/retinanet.py(235)training_step()
-> losses = self.model(x,y)
(Pdb) tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5959e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.6665e-01, 0.0000e+00, 1.1933e-02, 0.0000e+00, 0.0000e+00,
        6.8948e-01, 2.2912e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3043e-01,
        0.0000e+00, 0.0000e+00, 1.4253e-01, 4.1802e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1237e-01, 0.0000e+00,
        9.4002e-03, 0.0000e+00, 0.0000e+00, 9.3810e-03, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.4949e-01, 0.0000e+00, 5.1469e-01, 1.0878e+00, 0.0000e+00,
        1.2035e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        4.3178e-01, 0.0000e+00, 0.0000e+00, 2.6768e-01, 1.4627e-02, 0.0000e+00,
        0.0000e+00, 7.5896e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0960e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4731e-01,
        0.0000e+00, 3.7174e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.9458e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5171e-01, 2.2435e+00, 0.0000e+00,
        1.3513e-02, 4.7942e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        4.4221e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        7.2300e-02, 0.0000e+00, 1.8908e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 9.1498e-02, 2.8973e-01, 0.0000e+00, 4.2655e-01,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0502e-01, 0.0000e+00,
        0.0000e+00, 3.8359e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        8.4389e-01, 8.9074e-01, 2.3667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        7.5424e-02, 2.4630e-03, 0.0000e+00, 3.7731e-02, 8.6682e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2547e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4328e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9876e-01, 6.7402e-01, 0.0000e+00,
        0.0000e+00, 1.4989e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        3.5461e-01, 0.0000e+00, 0.0000e+00, 6.4561e-04, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8861e-01, 9.0675e-02, 0.0000e+00,
        1.4720e-01, 0.0000e+00, 1.8280e+00, 1.6821e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 8.0749e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        2.9670e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5413e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7210e+00, 4.1359e-01,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        4.0453e-01, 2.5434e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        4.6714e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3194e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 5.5081e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 4.7321e-01, 3.0953e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.4466e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        6.3489e-01, 3.1963e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.0865e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.6048e-01, 0.0000e+00, 0.0000e+00, 1.0504e+00, 1.9350e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.6431e-01, 0.0000e+00,
        1.3130e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5027e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 9.7358e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.5333e-01, 0.0000e+00, 5.2666e-01, 3.1628e-01,
        0.0000e+00, 1.9284e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2449e-01,
        1.0573e+00, 0.0000e+00, 7.5686e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9511e-01,
        0.0000e+00, 0.0000e+00, 8.2818e-02, 0.0000e+00, 7.5956e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 6.1860e-01, 0.0000e+00, 2.9757e-01, 0.0000e+00,
        1.3737e-01, 1.4973e-02, 5.1716e-01, 1.4662e-01, 0.0000e+00, 2.1170e-01,
        1.2885e+00, 3.3365e-02, 0.0000e+00, 4.3161e-02, 0.0000e+00, 2.6681e-01,
        6.5359e-01, 8.4975e-02, 0.0000e+00, 4.9648e-02, 1.8812e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 7.8791e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 9.1632e-02, 1.1392e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 7.7443e-03, 3.1854e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0065e-01, 5.7093e-02, 0.0000e+00,
        1.2925e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7604e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1548e-01, 0.0000e+00, 1.0658e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.2002e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.1041e+00, 0.0000e+00, 1.0001e+00, 9.9760e-01, 0.0000e+00, 3.9286e-01,
        5.2747e-01, 1.0813e-01, 1.3240e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.2424e-01, 0.0000e+00, 0.0000e+00, 4.7743e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0327e-04, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.5978e-01, 1.2669e-01, 1.8486e+00, 0.0000e+00,
        1.7589e-01, 0.0000e+00, 6.1413e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        7.8575e-01, 0.0000e+00, 1.7464e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        9.2673e-01, 0.0000e+00, 0.0000e+00, 4.8677e-01, 0.0000e+00, 0.0000e+00,
        1.0039e-01, 0.0000e+00, 0.0000e+00, 1.7821e-02, 6.2813e-01, 0.0000e+00,
        1.9629e+00, 3.6478e-02, 0.0000e+00, 2.1001e-01, 2.8245e-01, 2.3825e-01,
        9.0256e-02, 1.6250e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8359e+00,
        5.2149e-01, 0.0000e+00, 1.3081e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2385e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.3852e-02, 5.0324e-02, 6.3259e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        4.3538e-02, 0.0000e+00, 0.0000e+00, 7.0207e-01, 3.7859e-01, 5.7720e-01,
        0.0000e+00, 0.0000e+00, 8.1663e-01, 0.0000e+00, 0.0000e+00, 4.1615e-01,
        0.0000e+00, 0.0000e+00], dtype=torch.float64, requires_grad=True)
(Pdb) *** RuntimeError: A view was created in no_grad mode and its base or another view of its base has been modified inplace with grad mode enabled. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.
(Pdb) tensor([0.0000e+00, 0.0000e+00, 5.6486e-01, 0.0000e+00, 9.7720e-04, 0.0000e+00,
        0.0000e+00, 3.3600e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5610e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.0952e-02, 1.3630e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 4.4340e-01, 0.0000e+00, 2.7884e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0037e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.2587e-01, 0.0000e+00, 0.0000e+00, 1.7111e+00, 0.0000e+00,
        4.1283e-01, 1.6071e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5771e-01, 1.4905e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3910e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.3436e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.8732e-03, 0.0000e+00, 0.0000e+00, 3.5155e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 9.5058e-03, 0.0000e+00, 1.5320e-02, 3.2439e-01, 0.0000e+00,
        0.0000e+00, 7.9584e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.6707e-02, 0.0000e+00, 8.3296e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 7.2825e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 4.1724e-01, 2.4584e-01, 3.8423e-01, 7.5054e-03,
        0.0000e+00, 1.1839e+00, 4.7646e-02, 0.0000e+00, 1.0028e+00, 0.0000e+00,
        0.0000e+00, 1.6598e-02, 3.6233e-01, 0.0000e+00, 2.5988e-01, 7.7398e-03,
        1.0546e+00, 9.3730e-01, 5.6525e-01, 0.0000e+00, 0.0000e+00, 2.6005e-01,
        6.7497e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1846e-01, 0.0000e+00,
        0.0000e+00, 2.0009e-01, 0.0000e+00, 0.0000e+00, 1.0877e+00, 0.0000e+00,
        0.0000e+00, 3.7794e-01, 0.0000e+00, 3.9357e-03, 3.3493e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.2152e-02, 0.0000e+00, 3.6205e-01, 0.0000e+00,
        0.0000e+00, 2.9279e-01, 0.0000e+00, 0.0000e+00, 1.9994e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        6.2435e-02, 1.9683e-02, 0.0000e+00, 3.8492e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.2627e+00, 6.8352e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.7157e-01, 0.0000e+00, 0.0000e+00, 1.2075e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1736e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.1952e-01, 0.0000e+00, 7.0322e-01, 4.7335e-01,
        0.0000e+00, 6.2843e-01, 0.0000e+00, 9.5868e-02, 1.4061e+00, 3.2528e-01,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        8.7401e-02, 0.0000e+00, 1.3968e-01, 0.0000e+00, 3.7667e-02, 0.0000e+00,
        5.1813e-01, 4.7483e-01, 0.0000e+00, 0.0000e+00, 2.9226e-01, 0.0000e+00,
        0.0000e+00, 1.2305e-01, 0.0000e+00, 2.0261e-01, 6.0026e-02, 1.0085e+00,
        0.0000e+00, 1.4452e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5717e-01, 1.5029e+00,
        4.3539e-01, 7.8917e-02, 1.9142e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.6282e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 4.8178e-03, 0.0000e+00, 0.0000e+00, 4.4910e-01, 0.0000e+00,
        0.0000e+00, 4.1372e-01, 0.0000e+00, 0.0000e+00, 1.6993e+00, 0.0000e+00,
        3.5446e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1119e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.9899e+00, 0.0000e+00, 1.6027e+00, 7.8987e-04,
        4.1296e-02, 1.4273e+00, 0.0000e+00, 9.0462e-02, 2.1265e-01, 4.7015e-01,
        1.4493e+00, 0.0000e+00, 1.0066e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.7492e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.8558e-01, 0.0000e+00, 6.5020e-01, 0.0000e+00, 5.6634e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.3163e+00, 5.7428e-01, 5.8331e-01, 5.2840e-01,
        0.0000e+00, 3.5018e-02, 1.0373e+00, 7.5098e-02, 4.2691e-02, 1.4728e+00,
        9.1642e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4155e-01,
        9.9553e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4459e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.7028e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 8.9671e-01, 0.0000e+00, 1.3114e-02, 1.9441e-01, 0.0000e+00,
        1.4403e-02, 4.8905e-01, 0.0000e+00, 7.1332e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3628e-01, 0.0000e+00, 0.0000e+00,
        1.7035e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6735e-02, 0.0000e+00,
        2.4069e-01, 8.2618e-01, 0.0000e+00, 8.1662e-02, 0.0000e+00, 1.0489e-02,
        0.0000e+00, 4.1119e+00, 0.0000e+00, 2.7507e-01, 0.0000e+00, 3.9473e-01,
        0.0000e+00, 3.3967e-02, 1.0322e+00, 0.0000e+00, 0.0000e+00, 1.2159e-03,
        2.0854e-01, 0.0000e+00, 0.0000e+00, 1.4869e+00, 0.0000e+00, 0.0000e+00,
        1.7695e-01, 3.3024e-02, 6.1789e-01, 0.0000e+00, 2.3612e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.4368e-03, 0.0000e+00, 1.5623e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2048e-02, 3.7135e-02,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6261e-02, 0.0000e+00, 0.0000e+00,
        1.1605e-01, 3.7282e-01, 2.6238e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.4303e+00, 0.0000e+00, 0.0000e+00, 4.9590e-01, 5.2212e-02, 0.0000e+00,
        7.7748e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1797e-02, 2.2745e-02,
        1.3561e+00, 1.3715e-01, 7.4849e-01, 0.0000e+00, 9.1447e-02, 3.2168e-01,
        0.0000e+00, 1.8156e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5158e-01,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.7393e-01, 0.0000e+00, 0.0000e+00, 4.0947e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.5988e-02, 0.0000e+00, 4.6531e-02, 6.3466e-01, 1.0928e+00,
        0.0000e+00, 0.0000e+00, 3.3352e-01, 0.0000e+00, 0.0000e+00, 3.9178e-02,
        0.0000e+00, 0.0000e+00], dtype=torch.float64, requires_grad=True)
(Pdb) tensor([391, 128, 430, 202, 430, 202, 430, 202, 202, 202, 104, 296, 296, 296,
        202, 104, 202, 430, 202, 202, 430, 168, 277, 277, 430, 104, 356, 241,
        391, 430, 202, 202, 296, 346, 430, 168, 501, 430, 176, 430, 430,  88,
        277,  88, 202, 430, 220, 169, 430, 391, 326, 202, 241, 296, 430, 391,
        391, 296, 168, 346, 346, 168, 202, 430, 391, 296, 296, 202, 430, 202,
        356, 391, 430,  88, 132,  88, 252, 296, 430, 202, 202, 202, 346, 430,
        282, 391, 202, 391, 202, 202, 202, 346, 104, 176, 104, 296, 202, 202,
        176, 176, 430, 296, 391, 391, 430, 277, 296, 104,  40,  88,  88,  73,
        202, 104, 346, 296, 306, 176, 430, 202, 391, 306, 430, 202, 430, 202,
        226, 202, 391, 391, 202, 202, 391, 202, 202, 202, 391, 391, 391, 202,
        356])
(Pdb) Epoch 0:   0%|          | 0/534 [01:42<?, ?it/s]
Traceback (most recent call last):
  File "train.py", line 78, in <module>
    main(args)
  File "train.py", line 59, in main
    trainer.fit(model, dm)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 499, in fit
    self.dispatch()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 546, in dispatch
    self.accelerator.start_training(self)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 114, in start_training
    self._results = trainer.run_train()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 637, in run_train
    self.train_loop.run_training_epoch()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 492, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 654, in run_training_batch
    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 433, in optimizer_step
    using_lbfgs=is_lbfgs,
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1390, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py", line 214, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py", line 134, in __optimizer_step
    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 277, in optimizer_step
    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 282, in run_optimizer_step
    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=lambda_closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/optim/optimizer.py", line 89, in wrapper
    return func(*args, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/optim/adam.py", line 66, in step
    loss = closure()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 649, in train_step_and_backward_closure
    split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 742, in training_step_and_backward
    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 293, in training_step
    training_step_output = self.trainer.accelerator.training_step(args)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 156, in training_step
    return self.training_type_plugin.training_step(*args)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 125, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/Users/emilecarron/projects/thesis/detectie/retinanet.py", line 235, in training_step
    losses = self.model(x,y)
  File "/Users/emilecarron/projects/thesis/detectie/retinanet.py", line 235, in training_step
    losses = self.model(x,y)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/bdb.py", line 51, in trace_dispatch
    return self.dispatch_line(frame)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/bdb.py", line 70, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
