GPU available: False, used: False
TPU available: False, using: 0 TPU cores

  | Name             | Type             | Params
------------------------------------------------------
0 | anchor_generator | AnchorGenerator  | 0     
1 | backbone         | BackboneWithFPN  | 14.3 M
2 | model            | RetinaNet        | 23.2 M
3 | bbone            | ResNet           | 11.7 M
4 | extractor        | Sequential       | 11.2 M
5 | teacher_model    | RecognitionModel | 11.3 M
6 | tm               | Sequential       | 11.2 M
------------------------------------------------------
46.2 M    Trainable params
0         Non-trainable params
46.2 M    Total params
184.650   Total estimated model params size (MB)
/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/Users/emilecarron/dataset/SKU110K/images/val_169.jpg
Validation sanity check:  50%|█████     | 1/2 [00:13<00:13, 13.13s/it]/Users/emilecarron/dataset/SKU110K/images/val_33.jpg
Validation sanity check: 100%|██████████| 2/2 [00:25<00:00, 12.76s/it]/Users/emilecarron/dataset/SKU110K/images/val_280.jpg
                                                                      /Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/534 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/534 [00:00<?, ?it/s] /Users/emilecarron/dataset/SKU110K/images/train_0.jpg
/Users/emilecarron/dataset/SKU110K/images/train_1.jpg
> /Users/emilecarron/projects/thesis/detectie/retinanet.py(194)training_step()
-> x, y = batch
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(195)training_step()
-> y = [{'boxes': b, 'labels': l, 'embedding': e}
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(196)training_step()
-> for b, l, e in zip(y['boxes'],y['labels'], y['embedding'])
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(199)training_step()
-> boxes = y[0]['boxes'].int()
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(200)training_step()
-> counter = 0
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(201)training_step()
-> for idx in boxes:
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(202)training_step()
-> height = idx[3]-idx[1]
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(203)training_step()
-> width = idx[2]-idx[0]
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(204)training_step()
-> if height < 7:
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(207)training_step()
-> if width < 7:
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(211)training_step()
-> image = torchvision.transforms.functional.crop(x, idx[1], idx[0], height, width)
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(212)training_step()
-> self.tm.eval()
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(213)training_step()
-> predictions = self.tm(image)
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(216)training_step()
-> _, predicted = torch.max(predictions.data, 1)
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(217)training_step()
-> predictions = torch.squeeze(predictions)
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(218)training_step()
-> y[0]['labels'][counter] = predicted
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(219)training_step()
-> with torch.no_grad():
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(221)training_step()
-> y[0]['embedding'][counter] = predictions
(Pdb) tensor([0.0000e+00, 0.0000e+00, 4.2154e-01, 0.0000e+00, 4.2753e-02, 0.0000e+00,
        0.0000e+00, 5.9052e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0624e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 4.7766e-02, 1.9524e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.5464e-01, 0.0000e+00, 6.1736e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8308e-01, 1.0860e-02, 0.0000e+00,
        0.0000e+00, 6.5387e-02, 0.0000e+00, 0.0000e+00, 1.6791e+00, 0.0000e+00,
        7.8169e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0726e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7908e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9255e-01,
        0.0000e+00, 9.2448e-01, 0.0000e+00, 9.1684e-02, 0.0000e+00, 0.0000e+00,
        2.6379e-02, 0.0000e+00, 0.0000e+00, 4.3094e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 6.5909e-02, 2.8637e-02, 5.0278e-02, 1.9098e-01, 0.0000e+00,
        0.0000e+00, 7.6047e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        3.6009e-02, 0.0000e+00, 1.2533e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.0673e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.8066e-01, 1.7238e-01, 2.6023e-01, 6.7863e-02,
        0.0000e+00, 6.3497e-01, 0.0000e+00, 0.0000e+00, 1.0823e+00, 0.0000e+00,
        0.0000e+00, 1.4888e-01, 3.3533e-01, 0.0000e+00, 5.1852e-01, 0.0000e+00,
        7.0001e-01, 7.6221e-01, 7.6734e-01, 0.0000e+00, 0.0000e+00, 2.4381e-01,
        8.5312e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8840e-02, 0.0000e+00,
        0.0000e+00, 1.7028e-01, 0.0000e+00, 0.0000e+00, 1.6727e+00, 0.0000e+00,
        0.0000e+00, 3.2566e-01, 0.0000e+00, 0.0000e+00, 9.7344e-02, 1.4460e-01,
        0.0000e+00, 0.0000e+00, 2.1235e-02, 1.0915e-01, 7.0394e-01, 0.0000e+00,
        0.0000e+00, 3.7786e-01, 0.0000e+00, 0.0000e+00, 2.1011e-02, 0.0000e+00,
        6.3295e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.4588e-03, 0.0000e+00, 2.5211e-01, 5.2046e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.6568e+00, 5.9466e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 6.9599e-01, 0.0000e+00, 0.0000e+00, 7.4798e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.9816e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 7.6762e-02, 0.0000e+00, 7.8353e-01, 1.9188e-01,
        0.0000e+00, 3.0478e-01, 0.0000e+00, 0.0000e+00, 1.8385e+00, 1.7380e-01,
        0.0000e+00, 4.1998e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        3.4625e-01, 0.0000e+00, 4.0727e-02, 0.0000e+00, 1.2595e-01, 0.0000e+00,
        2.8024e-01, 4.1090e-02, 1.8958e-01, 0.0000e+00, 2.3406e-01, 0.0000e+00,
        0.0000e+00, 1.5436e-01, 0.0000e+00, 0.0000e+00, 2.0847e-02, 5.7256e-01,
        0.0000e+00, 2.6280e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.4018e-02, 1.3040e+00,
        8.4684e-01, 6.4186e-02, 1.7434e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1258e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 5.8478e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 5.8319e-02, 0.0000e+00, 0.0000e+00, 4.2700e-01, 1.6324e-03,
        0.0000e+00, 1.0771e-01, 0.0000e+00, 0.0000e+00, 1.1789e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8667e-02,
        0.0000e+00, 1.3337e-02, 0.0000e+00, 2.1851e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.8786e+00, 0.0000e+00, 1.0964e+00, 2.1105e-01,
        7.5993e-02, 1.2433e+00, 0.0000e+00, 4.0312e-02, 5.2159e-01, 6.3580e-01,
        1.7091e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.6361e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9599e-02,
        5.0386e-02, 0.0000e+00, 5.7254e-01, 0.0000e+00, 8.0069e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 9.0628e-01, 1.3474e-01, 7.4393e-01, 2.3351e-01,
        0.0000e+00, 0.0000e+00, 6.8718e-01, 4.5576e-01, 7.1350e-03, 1.5367e+00,
        8.6992e-02, 0.0000e+00, 0.0000e+00, 6.4335e-04, 0.0000e+00, 3.4243e-01,
        6.8782e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7987e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.1742e+00, 0.0000e+00, 0.0000e+00, 4.9447e-02,
        0.0000e+00, 4.1221e-01, 1.7342e-02, 7.7541e-02, 2.0189e-01, 0.0000e+00,
        0.0000e+00, 6.0107e-01, 0.0000e+00, 4.0710e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5020e-01, 1.5146e-01, 0.0000e+00,
        1.4901e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2786e-01, 0.0000e+00,
        4.9622e-01, 7.5835e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.6444e+00, 0.0000e+00, 2.2580e-01, 0.0000e+00, 1.8204e-01,
        0.0000e+00, 1.9032e-01, 8.2655e-01, 0.0000e+00, 0.0000e+00, 6.3868e-02,
        4.5809e-01, 0.0000e+00, 0.0000e+00, 1.4081e+00, 0.0000e+00, 0.0000e+00,
        1.0194e-01, 9.9083e-02, 6.9492e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2282e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3529e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.4421e-02, 0.0000e+00, 2.1456e-01, 3.5370e-02,
        0.0000e+00, 0.0000e+00, 8.4011e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.3530e-01, 2.7076e-01, 1.4295e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.7161e+00, 0.0000e+00, 0.0000e+00, 6.2533e-01, 1.3914e-01, 0.0000e+00,
        3.0553e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.0094e+00, 7.9069e-02, 7.7605e-01, 0.0000e+00, 9.3684e-02, 1.1687e-01,
        0.0000e+00, 4.9671e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1012e-01,
        3.9716e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.2075e-01, 0.0000e+00, 0.0000e+00, 1.0850e-01, 2.5725e-01,
        0.0000e+00, 0.0000e+00, 1.1828e-02, 0.0000e+00, 3.1087e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 8.8603e-02, 0.0000e+00, 1.4018e-01, 9.8764e-01, 8.8363e-01,
        0.0000e+00, 0.0000e+00, 6.9276e-01, 0.0000e+00, 0.0000e+00, 7.3088e-02,
        0.0000e+00, 0.0000e+00], grad_fn=<SqueezeBackward0>)
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(201)training_step()
-> for idx in boxes:
(Pdb) tensor(391)
(Pdb) *** RuntimeError: Output 0 of UnbindBackward is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.
(Pdb) *** RuntimeError: Output 0 of UnbindBackward is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(202)training_step()
-> height = idx[3]-idx[1]
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(203)training_step()
-> width = idx[2]-idx[0]
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(204)training_step()
-> if height < 7:
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(207)training_step()
-> if width < 7:
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(211)training_step()
-> image = torchvision.transforms.functional.crop(x, idx[1], idx[0], height, width)
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(212)training_step()
-> self.tm.eval()
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(213)training_step()
-> predictions = self.tm(image)
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(216)training_step()
-> _, predicted = torch.max(predictions.data, 1)
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(217)training_step()
-> predictions = torch.squeeze(predictions)
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(218)training_step()
-> y[0]['labels'][counter] = predicted
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(219)training_step()
-> with torch.no_grad():
(Pdb) > /Users/emilecarron/projects/thesis/detectie/retinanet.py(221)training_step()
-> y[0]['embedding'][counter] = predictions
(Pdb) tensor([0.0000e+00, 0.0000e+00, 4.2154e-01, 0.0000e+00, 4.2753e-02, 0.0000e+00,
        0.0000e+00, 5.9052e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0624e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 4.7766e-02, 1.9524e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.5464e-01, 0.0000e+00, 6.1736e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8308e-01, 1.0860e-02, 0.0000e+00,
        0.0000e+00, 6.5387e-02, 0.0000e+00, 0.0000e+00, 1.6791e+00, 0.0000e+00,
        7.8169e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0726e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7908e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9255e-01,
        0.0000e+00, 9.2448e-01, 0.0000e+00, 9.1684e-02, 0.0000e+00, 0.0000e+00,
        2.6379e-02, 0.0000e+00, 0.0000e+00, 4.3094e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 6.5909e-02, 2.8637e-02, 5.0278e-02, 1.9098e-01, 0.0000e+00,
        0.0000e+00, 7.6047e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        3.6009e-02, 0.0000e+00, 1.2533e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.0673e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.8066e-01, 1.7238e-01, 2.6023e-01, 6.7863e-02,
        0.0000e+00, 6.3497e-01, 0.0000e+00, 0.0000e+00, 1.0823e+00, 0.0000e+00,
        0.0000e+00, 1.4888e-01, 3.3533e-01, 0.0000e+00, 5.1852e-01, 0.0000e+00,
        7.0001e-01, 7.6221e-01, 7.6734e-01, 0.0000e+00, 0.0000e+00, 2.4381e-01,
        8.5312e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8840e-02, 0.0000e+00,
        0.0000e+00, 1.7028e-01, 0.0000e+00, 0.0000e+00, 1.6727e+00, 0.0000e+00,
        0.0000e+00, 3.2566e-01, 0.0000e+00, 0.0000e+00, 9.7344e-02, 1.4460e-01,
        0.0000e+00, 0.0000e+00, 2.1235e-02, 1.0915e-01, 7.0394e-01, 0.0000e+00,
        0.0000e+00, 3.7786e-01, 0.0000e+00, 0.0000e+00, 2.1011e-02, 0.0000e+00,
        6.3295e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.4588e-03, 0.0000e+00, 2.5211e-01, 5.2046e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.6568e+00, 5.9466e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 6.9599e-01, 0.0000e+00, 0.0000e+00, 7.4798e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.9816e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 7.6762e-02, 0.0000e+00, 7.8353e-01, 1.9188e-01,
        0.0000e+00, 3.0478e-01, 0.0000e+00, 0.0000e+00, 1.8385e+00, 1.7380e-01,
        0.0000e+00, 4.1998e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        3.4625e-01, 0.0000e+00, 4.0727e-02, 0.0000e+00, 1.2595e-01, 0.0000e+00,
        2.8024e-01, 4.1090e-02, 1.8958e-01, 0.0000e+00, 2.3406e-01, 0.0000e+00,
        0.0000e+00, 1.5436e-01, 0.0000e+00, 0.0000e+00, 2.0847e-02, 5.7256e-01,
        0.0000e+00, 2.6280e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.4018e-02, 1.3040e+00,
        8.4684e-01, 6.4186e-02, 1.7434e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1258e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 5.8478e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 5.8319e-02, 0.0000e+00, 0.0000e+00, 4.2700e-01, 1.6324e-03,
        0.0000e+00, 1.0771e-01, 0.0000e+00, 0.0000e+00, 1.1789e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8667e-02,
        0.0000e+00, 1.3337e-02, 0.0000e+00, 2.1851e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.8786e+00, 0.0000e+00, 1.0964e+00, 2.1105e-01,
        7.5993e-02, 1.2433e+00, 0.0000e+00, 4.0312e-02, 5.2159e-01, 6.3580e-01,
        1.7091e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.6361e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9599e-02,
        5.0386e-02, 0.0000e+00, 5.7254e-01, 0.0000e+00, 8.0069e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 9.0628e-01, 1.3474e-01, 7.4393e-01, 2.3351e-01,
        0.0000e+00, 0.0000e+00, 6.8718e-01, 4.5576e-01, 7.1350e-03, 1.5367e+00,
        8.6992e-02, 0.0000e+00, 0.0000e+00, 6.4335e-04, 0.0000e+00, 3.4243e-01,
        6.8782e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7987e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.1742e+00, 0.0000e+00, 0.0000e+00, 4.9447e-02,
        0.0000e+00, 4.1221e-01, 1.7342e-02, 7.7541e-02, 2.0189e-01, 0.0000e+00,
        0.0000e+00, 6.0107e-01, 0.0000e+00, 4.0710e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5020e-01, 1.5146e-01, 0.0000e+00,
        1.4901e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2786e-01, 0.0000e+00,
        4.9622e-01, 7.5835e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.6444e+00, 0.0000e+00, 2.2580e-01, 0.0000e+00, 1.8204e-01,
        0.0000e+00, 1.9032e-01, 8.2655e-01, 0.0000e+00, 0.0000e+00, 6.3868e-02,
        4.5809e-01, 0.0000e+00, 0.0000e+00, 1.4081e+00, 0.0000e+00, 0.0000e+00,
        1.0194e-01, 9.9083e-02, 6.9492e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2282e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3529e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.4421e-02, 0.0000e+00, 2.1456e-01, 3.5370e-02,
        0.0000e+00, 0.0000e+00, 8.4011e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.3530e-01, 2.7076e-01, 1.4295e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.7161e+00, 0.0000e+00, 0.0000e+00, 6.2533e-01, 1.3914e-01, 0.0000e+00,
        3.0553e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.0094e+00, 7.9069e-02, 7.7605e-01, 0.0000e+00, 9.3684e-02, 1.1687e-01,
        0.0000e+00, 4.9671e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1012e-01,
        3.9716e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.2075e-01, 0.0000e+00, 0.0000e+00, 1.0850e-01, 2.5725e-01,
        0.0000e+00, 0.0000e+00, 1.1828e-02, 0.0000e+00, 3.1087e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 8.8603e-02, 0.0000e+00, 1.4018e-01, 9.8764e-01, 8.8363e-01,
        0.0000e+00, 0.0000e+00, 6.9276e-01, 0.0000e+00, 0.0000e+00, 7.3088e-02,
        0.0000e+00, 0.0000e+00], dtype=torch.float64, requires_grad=True)
(Pdb) tensor([0.0000e+00, 0.0000e+00, 4.2154e-01, 0.0000e+00, 4.2753e-02, 0.0000e+00,
        0.0000e+00, 5.9052e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0624e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 4.7766e-02, 1.9524e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.5464e-01, 0.0000e+00, 6.1736e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8308e-01, 1.0860e-02, 0.0000e+00,
        0.0000e+00, 6.5387e-02, 0.0000e+00, 0.0000e+00, 1.6791e+00, 0.0000e+00,
        7.8169e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0726e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7908e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9255e-01,
        0.0000e+00, 9.2448e-01, 0.0000e+00, 9.1684e-02, 0.0000e+00, 0.0000e+00,
        2.6379e-02, 0.0000e+00, 0.0000e+00, 4.3094e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 6.5909e-02, 2.8637e-02, 5.0278e-02, 1.9098e-01, 0.0000e+00,
        0.0000e+00, 7.6047e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        3.6009e-02, 0.0000e+00, 1.2533e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.0673e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.8066e-01, 1.7238e-01, 2.6023e-01, 6.7863e-02,
        0.0000e+00, 6.3497e-01, 0.0000e+00, 0.0000e+00, 1.0823e+00, 0.0000e+00,
        0.0000e+00, 1.4888e-01, 3.3533e-01, 0.0000e+00, 5.1852e-01, 0.0000e+00,
        7.0001e-01, 7.6221e-01, 7.6734e-01, 0.0000e+00, 0.0000e+00, 2.4381e-01,
        8.5312e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8840e-02, 0.0000e+00,
        0.0000e+00, 1.7028e-01, 0.0000e+00, 0.0000e+00, 1.6727e+00, 0.0000e+00,
        0.0000e+00, 3.2566e-01, 0.0000e+00, 0.0000e+00, 9.7344e-02, 1.4460e-01,
        0.0000e+00, 0.0000e+00, 2.1235e-02, 1.0915e-01, 7.0394e-01, 0.0000e+00,
        0.0000e+00, 3.7786e-01, 0.0000e+00, 0.0000e+00, 2.1011e-02, 0.0000e+00,
        6.3295e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.4588e-03, 0.0000e+00, 2.5211e-01, 5.2046e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.6568e+00, 5.9466e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 6.9599e-01, 0.0000e+00, 0.0000e+00, 7.4798e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.9816e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 7.6762e-02, 0.0000e+00, 7.8353e-01, 1.9188e-01,
        0.0000e+00, 3.0478e-01, 0.0000e+00, 0.0000e+00, 1.8385e+00, 1.7380e-01,
        0.0000e+00, 4.1998e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        3.4625e-01, 0.0000e+00, 4.0727e-02, 0.0000e+00, 1.2595e-01, 0.0000e+00,
        2.8024e-01, 4.1090e-02, 1.8958e-01, 0.0000e+00, 2.3406e-01, 0.0000e+00,
        0.0000e+00, 1.5436e-01, 0.0000e+00, 0.0000e+00, 2.0847e-02, 5.7256e-01,
        0.0000e+00, 2.6280e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.4018e-02, 1.3040e+00,
        8.4684e-01, 6.4186e-02, 1.7434e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1258e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 5.8478e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 5.8319e-02, 0.0000e+00, 0.0000e+00, 4.2700e-01, 1.6324e-03,
        0.0000e+00, 1.0771e-01, 0.0000e+00, 0.0000e+00, 1.1789e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8667e-02,
        0.0000e+00, 1.3337e-02, 0.0000e+00, 2.1851e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 2.8786e+00, 0.0000e+00, 1.0964e+00, 2.1105e-01,
        7.5993e-02, 1.2433e+00, 0.0000e+00, 4.0312e-02, 5.2159e-01, 6.3580e-01,
        1.7091e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.6361e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9599e-02,
        5.0386e-02, 0.0000e+00, 5.7254e-01, 0.0000e+00, 8.0069e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 9.0628e-01, 1.3474e-01, 7.4393e-01, 2.3351e-01,
        0.0000e+00, 0.0000e+00, 6.8718e-01, 4.5576e-01, 7.1350e-03, 1.5367e+00,
        8.6992e-02, 0.0000e+00, 0.0000e+00, 6.4335e-04, 0.0000e+00, 3.4243e-01,
        6.8782e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7987e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.1742e+00, 0.0000e+00, 0.0000e+00, 4.9447e-02,
        0.0000e+00, 4.1221e-01, 1.7342e-02, 7.7541e-02, 2.0189e-01, 0.0000e+00,
        0.0000e+00, 6.0107e-01, 0.0000e+00, 4.0710e-01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5020e-01, 1.5146e-01, 0.0000e+00,
        1.4901e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2786e-01, 0.0000e+00,
        4.9622e-01, 7.5835e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 3.6444e+00, 0.0000e+00, 2.2580e-01, 0.0000e+00, 1.8204e-01,
        0.0000e+00, 1.9032e-01, 8.2655e-01, 0.0000e+00, 0.0000e+00, 6.3868e-02,
        4.5809e-01, 0.0000e+00, 0.0000e+00, 1.4081e+00, 0.0000e+00, 0.0000e+00,
        1.0194e-01, 9.9083e-02, 6.9492e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2282e-02, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3529e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 3.4421e-02, 0.0000e+00, 2.1456e-01, 3.5370e-02,
        0.0000e+00, 0.0000e+00, 8.4011e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.3530e-01, 2.7076e-01, 1.4295e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.7161e+00, 0.0000e+00, 0.0000e+00, 6.2533e-01, 1.3914e-01, 0.0000e+00,
        3.0553e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.0094e+00, 7.9069e-02, 7.7605e-01, 0.0000e+00, 9.3684e-02, 1.1687e-01,
        0.0000e+00, 4.9671e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1012e-01,
        3.9716e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.2075e-01, 0.0000e+00, 0.0000e+00, 1.0850e-01, 2.5725e-01,
        0.0000e+00, 0.0000e+00, 1.1828e-02, 0.0000e+00, 3.1087e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 8.8603e-02, 0.0000e+00, 1.4018e-01, 9.8764e-01, 8.8363e-01,
        0.0000e+00, 0.0000e+00, 6.9276e-01, 0.0000e+00, 0.0000e+00, 7.3088e-02,
        0.0000e+00, 0.0000e+00], dtype=torch.float64, requires_grad=True)
(Pdb) *** SyntaxError: invalid syntax
(Pdb) Epoch 0:   0%|          | 0/534 [02:59<?, ?it/s]
Traceback (most recent call last):
  File "train.py", line 78, in <module>
    main(args)
  File "train.py", line 59, in main
    trainer.fit(model, dm)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 499, in fit
    self.dispatch()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 546, in dispatch
    self.accelerator.start_training(self)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 114, in start_training
    self._results = trainer.run_train()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 637, in run_train
    self.train_loop.run_training_epoch()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 492, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 654, in run_training_batch
    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 433, in optimizer_step
    using_lbfgs=is_lbfgs,
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1390, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py", line 214, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py", line 134, in __optimizer_step
    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 277, in optimizer_step
    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 282, in run_optimizer_step
    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=lambda_closure, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/optim/optimizer.py", line 89, in wrapper
    return func(*args, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/optim/adam.py", line 66, in step
    loss = closure()
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 649, in train_step_and_backward_closure
    split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 742, in training_step_and_backward
    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 293, in training_step
    training_step_output = self.trainer.accelerator.training_step(args)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 156, in training_step
    return self.training_type_plugin.training_step(*args)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 125, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/Users/emilecarron/projects/thesis/detectie/retinanet.py", line 221, in training_step
    #test = torch.no_grad(predictions)
  File "/Users/emilecarron/projects/thesis/detectie/retinanet.py", line 221, in training_step
    #test = torch.no_grad(predictions)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/bdb.py", line 51, in trace_dispatch
    return self.dispatch_line(frame)
  File "/Users/emilecarron/opt/anaconda3/envs/thesis/lib/python3.6/bdb.py", line 70, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
